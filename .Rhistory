baseRMS[2] <- mean((test$logSalary-pred.hitters)^2)
baseRMS[2]
rf.hitters=randomForest(logSalary~.,data=hitters,subset=train_ind,importance=TRUE)
pred.hitters = predict(rf.hitters, hitters[-train_ind,],type="response")
baseRMS[3] <- mean((test$logSalary-pred.hitters)^2)
baseRMS[3]
d = 3; lambda = 0.01
B = c(10, 100, 200, 500, 1000, 3000)
par(mfrow=c(2,3))
# set plotting grid dims
for (j in seq(1,5)){
for(i in seq(1,length(B))){
boost.hitters=gbm(logSalary~.,data=hitters[train_ind,],distribution="gaussian", cv.folds=50, n.cores=2, interaction.depth=d,shrinkage=lambda, n.trees = B[i])
pred.hitters = predict(boost.hitters,newdata=hitters[-train_ind,],type="response",n.trees=B[i])
gbm.perf(boost.hitters,method="cv")+
title(B[i])
}
}
boost.hitters=gbm(logSalary~.,data=hitters[train_ind,],distribution="gaussian", cv.folds=50, n.cores=2, interaction.depth=d,shrinkage=lambda, n.trees = 3000)
pred.hitters = predict(boost.hitters,newdata=hitters[-train_ind,],type="response",n.trees=3000)
baseRMS[4] <- mean((test$logSalary-pred.hitters)^2)
baseRMS[4]
pairs(hitters[,c("AtBat", "Hits", "Runs", "logSalary")])
pairs(hitters[,c("CAtBat", "CHits", "CRuns", "logSalary")])
hittersMod <- subset(hitters, select = -c(AtBat, Runs, CAtBat, CRuns))
smp_size <- floor(0.70 * nrow(hittersMod))
trainMod <- hitters[train_indMod, ]
testMod <- hitters[-train_indMod, ]
tree.bb = tree(logSalary~Hits+Years,data=trainMod)
mean((testMod$logSalary-predict(tree.bb))^2)
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19)
prune.bb = prune.tree(tree.bb,best=4)
colinRMS[1] <- mean((testMod$logSalary-predict(prune.bb))^2)
colinRMS[1]
bag.hitters=randomForest(logSalary~.,data=hittersMod,subset=train_indMod,mtry=19,importance=TRUE)
pred.hitters = predict(bag.hitters, hittersMod[-train_indMod,],type="response")
colinRMS[2] <- mean((testMod$logSalary-pred.hitters)^2)
colinRMS[2]
rf.hitters=randomForest(logSalary~.,data=hittersMod,subset=train_indMod,importance=TRUE)
pred.hitters = predict(rf.hitters, hittersMod[-train_indMod,],type="response")
colinRMS[3] <- mean((testMod$logSalary-pred.hitters)^2)
colinRMS[3]
boost.hitters=gbm(logSalary~.,data=hittersMod[train_indMod,],distribution="gaussian", cv.folds=50, n.cores=2, interaction.depth=d,shrinkage=lambda, n.trees = 100)
pred.hitters = predict(boost.hitters,newdata=hittersMod[-train_indMod,],type="response",n.trees=3000)
colinRMS[4] <- mean((testMod$logSalary-pred.hitters)^2)
colinRMS[4]
baseRMS
colinRMS
baseRMS-colinRMS
barplot(baseRMS)
barplot(baseRMS, colinRMS)
barplot(c(baseRMS, colinRMS))
barplot(c(baseRMS, colinRMS), col = c("red", "blue"))
c(baseRMS[1], colinRMS[1])
barplot(c(baseRMS[1], colinRMS[1]))
barplot(aggregate(c(baseRMS[1], colinRMS[1])))
barplot(aggregate(cbind(baseRMS[1], colinRMS[1])))
barplot(c(baseRMS, colinRMS),beside=TRUE)
matrix(baseRMS,colinRMS)
matrix(c(baseRMS,colinRMS))
matrix(2:4, c(baseRMS,colinRMS))
cbind(baseRMS,colinRMS)
as.matrix(cbind(baseRMS,colinRMS))
barplot(cbind(baseRMS,colinRMS), beside = TRUE)
barplot(cbind(baseRMS,colinRMS), beside = TRUE)
barplot((cbind(baseRMS,colinRMS), beside = TRUE),T)
barplot(t(cbind(baseRMS,colinRMS), beside = TRUE))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE)
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE)+
labels(c("Tree", "Bagged", "Random", "Boosted"))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE)+
labels("Tree", "Bagged", "Random", "Boosted")
scores <- t(cbind(baseRMS,colinRMS)
scores
scores <- t(cbind(baseRMS,colinRMS)
scores
scores <- t(cbind(baseRMS,colinRMS))
scores
colnames(scores, c("Tree", "Bagged", "Random", "Boosted"))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted")
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE)
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE)+ labels(colnames(scores))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores))
+ legend(fill = c("base", "colinear removed"))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores))+
legend(fill = c("base", "colinear removed"))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores))+
legend(legend = "topright", fill = c("base", "colinear removed"))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores))+
legend(x = 0, y = 0, fill = c("base", "colinear removed"))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed"))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed"), xlim = 2)
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed"), ylim = 2)
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed"))
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after colinear variables are removed")
baseRMS-colinRMS
library(rpart)
md <- build.model(train = train, formula = logSalary~Hits+Years)
library(rpart)
build.model <- function(train, formula, maxdepth = 20){
rpart(formula,
data = train,
model = TRUE,
control = rpart.control(maxdepth = maxdepth,
minsplit = 2,
minbucket = 2,
cp = 0.0))
}
predict.model <- function(m, test){
predict(m, newdata = test, type = "vector")
}
md <- build.model(train = train, formula = logSalary~Hits+Years)
predict.model(m = md, test = test)
mean((test$logSalary-predict.model(m = md, test = test))^2)
library(rpart)
build.model <- function(train, formula, maxdepth = 20){
rpart(formula,
data = train,
model = TRUE,
control = rpart.control(maxdepth = maxdepth,
minsplit = 2,
minbucket = 2,
cp = 0.0))
}
predict.model <- function(m, test){
predict(m, newdata = test, type = "vector")
}
md <- build.model(train = train, formula = logSalary~Hits+Years)
predict.model(m = md, test = test)
baseRMS[5] <- mean((test$logSalary-predict.model(m = md, test = test))^2)
baseRMS[5]
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted")
library(rpart)
build.model <- function(train, formula, maxdepth = 20){
rpart(formula,
data = train,
model = TRUE,
control = rpart.control(maxdepth = maxdepth,
minsplit = 2,
minbucket = 2,
cp = 0.0))
}
predict.model <- function(m, test){
predict(m, newdata = test, type = "vector")
}
md <- build.model(train = train, formula = logSalary~Hits+Years)
predict.model(m = md, test = test)
baseRMS[5] <- mean((test$logSalary-predict.model(m = md, test = test))^2)
baseRMS[5]
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
colinRMS[5] <- NA
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
baseRMS[5]
library(rpart)
build.model <- function(train, formula, maxdepth = 20){
rpart(formula,
data = train,
model = TRUE,
control = rpart.control(maxdepth = maxdepth,
minsplit = 2,
minbucket = 2,
cp = 0.0))
}
predict.model <- function(m, test){
predict(m, newdata = test, type = "vector")
}
md <- build.model(train = train, formula = logSalary~Hits+Years)
predict.model(m = md, test = test)
baseRMS[5] <- mean((test$logSalary-predict.model(m = md, test = test))^2)
baseRMS[5]
colinRMS[5] <- NA
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
predict.model(m = md, test = test)
baseRMS[5] <- mean((test$logSalary-predict.model(m = md, test = test))^2)
library(rpart)
build.model <- function(train, formula, maxdepth = 20){
rpart(formula,
data = train,
model = TRUE,
control = rpart.control(maxdepth = maxdepth,
minsplit = 2,
minbucket = 2,
cp = 0.0))
}
predict.model <- function(m, test){
predict(m, newdata = test, type = "vector")
}
md <- build.model(train = train, formula = logSalary~Hits+Years)
# predict.model(m = md, test = test)
baseRMS[5] <- mean((test$logSalary-predict.model(m = md, test = test))^2)
baseRMS[5]
colinRMS[5] <- NA
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
library(ISLR)
library(tree)
library(randomForest)
library(gbm)
hitters <- force(Hitters)
hitters <- na.omit(hitters)
hitters$logSalary <- log(hitters$Salary)
hitters <- subset(hitters, select = -c(Salary))
baseRMS = vector(length = 5, mode = "numeric")
colinRMS = vector(length = 5, mode = "numeric")
smp_size <- floor(0.70 * nrow(hitters))
set.seed(345)
train_ind <- sample(seq_len(nrow(hitters)), size = smp_size)
train <- hitters[train_ind, ]
test <- hitters[-train_ind, ]
tree.bb = tree(logSalary~Hits+Years,data=trainMod)
mean((testMod$logSalary-predict(tree.bb))^2)
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19)
prune.bb = prune.tree(tree.bb,best=4)
colinRMS[1] <- mean((testMod$logSalary-predict(prune.bb))^2)
colinRMS[1]
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted")
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
scores
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
scores
barplot(scores), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
scores
barplot(scores, beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
tree.bb = tree(logSalary~Hits+Years,data=trainMod)
mean((testMod$logSalary-predict(tree.bb))^2)
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19)
prune.bb = prune.tree(tree.bb,best=4)
colinRMS[1] <- mean((testMod$logSalary-predict(prune.bb))^2)
colinRMS[1]
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
scores
barplot(scores, beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
scores
barplot(scores, beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
pairs(hitters[,c("AtBat", "Hits", "Runs", "logSalary")])
pairs(hitters[,c("CAtBat", "CHits", "CRuns", "logSalary")])
hittersMod <- subset(hitters, select = -c(AtBat, Runs, CAtBat, CRuns))
trainMod <- hitters[train_ind, ]
testMod <- hitters[-train_ind, ]
d = 3; lambda = 0.01
B = c(10, 100, 200, 500, 1000, 3000)
par(mfrow=c(2,3))
# set plotting grid dims
#for (j in seq(1,5)){
for(i in seq(1,length(B))){
boost.hitters=gbm(logSalary~.,data=hitters[train_ind,],distribution="gaussian", cv.folds=50, n.cores=2, interaction.depth=d,shrinkage=lambda, n.trees = B[i])
pred.hitters = predict(boost.hitters,newdata=hitters[-train_ind,],type="response",n.trees=B[i])
gbm.perf(boost.hitters,method="cv")+
title(B[i])
}
#}
knitr::opts_chunk$set(echo = TRUE)
tree.bb = tree(logSalary~Hits+Years,data=train)
library(ISLR)
library(tree)
library(randomForest)
library(gbm)
library(rpart)
library(rattle)					# Fancy tree plot
library(rpart.plot)				# Enhanced tree plots
library(RColorBrewer)
hitters <- force(Hitters)
hitters <- na.omit(hitters)
hitters$logSalary <- log(hitters$Salary)
hitters <- subset(hitters, select = -c(Salary))
baseRMS = vector(length = 5, mode = "numeric")
colinRMS = vector(length = 5, mode = "numeric")
smp_size <- floor(0.70 * nrow(hitters))
set.seed(345)
train_ind <- sample(seq_len(nrow(hitters)), size = smp_size)
train <- hitters[train_ind, ]
test <- hitters[-train_ind, ]
tree.bb = tree(logSalary~Hits+Years,data=train)
mean((test$logSalary-predict(tree.bb))^2)
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19)
prune.bb = prune.tree(tree.bb,best=4)
baseRMS[1] <- mean((test$logSalary-predict(prune.bb))^2)
plot(prune.bb)
text(prune.bb)
tree.bb = tree(logSalary~Hits+Years,data=train)
mean((test$logSalary-predict(tree.bb))^2)
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19)
prune.bb = prune.tree(tree.bb,best=4)
baseRMS[1] <- mean((test$logSalary-predict(prune.bb))^2)
baseRMS[1]
plot(prune.bb)
text(prune.bb)
fancyRpartPlot(prune.bb)
plot(prune.bb)
text(prune.bb)
tree.bb = tree(logSalary~Hits+Years,data=train)
mean((test$logSalary-predict(tree.bb))^2)
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19)
prune.bb = prune.tree(tree.bb,best=4)
baseRMS[1] <- mean((test$logSalary-predict(prune.bb))^2)
baseRMS[1]
plot(prune.bb)
text(prune.bb)
tree.bb = tree(logSalary~Hits+Years,data=train)
cat("unpruned tree RMS test error", mean((test$logSalary-predict(tree.bb))^2))
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19)
prune.bb = prune.tree(tree.bb,best=4)
baseRMS[1] <- mean((test$logSalary-predict(prune.bb))^2)
baseRMS[1]
plot(prune.bb)
text(prune.bb)
d = 3; lambda = 0.01
B = c(10, 100, 200, 500, 1000, 3000)
par(mfrow=c(2,3))
# set plotting grid dims
#for (j in seq(1,5)){
for(i in seq(1,length(B))){
boost.hitters=gbm(logSalary~.,data=hitters[train_ind,],distribution="gaussian", cv.folds=50, n.cores=2, interaction.depth=d,shrinkage=lambda, n.trees = B[i])
pred.hitters = predict(boost.hitters,newdata=hitters[-train_ind,],type="response",n.trees=B[i])
gbm.perf(boost.hitters,method="cv")+
title(B[i])
}
d = 3
lambda = 0.01
boost.hitters=gbm(logSalary~.,data=hitters[train_ind,],distribution="gaussian", cv.folds=50, n.cores=2, interaction.depth=d,shrinkage=lambda, n.trees = 3000)
d = 3
lambda = 0.01
boost.hitters=gbm(logSalary~.,data=hitters[train_ind,],distribution="gaussian", cv.folds=50, n.cores=2, interaction.depth=d,shrinkage=lambda, n.trees = 3000)
pred.hitters = predict(boost.hitters,newdata=hitters[-train_ind,],type="response",n.trees=3000)
gbm.perf(boost.hitters)
baseRMS[4] <- mean((test$logSalary-pred.hitters)^2)
gbm.perf(boost.hitters)
fancyRpartPlot(md)
# pre-pruning
# code from INFO304, which is teaching near identical content to STAT312
build.model <- function(train, formula, maxdepth = 20){
rpart(formula,
data = train,
model = TRUE,
control = rpart.control(maxdepth = maxdepth,
minsplit = 2,
minbucket = 2,
cp = 0.0))
}
predict.model <- function(m, test){
predict(m, newdata = test, type = "vector")
}
md <- build.model(train = train, formula = logSalary~Hits+Years)
fancyRpartPlot(md)
# predict.model(m = md, test = test)
baseRMS[5] <- mean((test$logSalary-predict.model(m = md, test = test))^2)
colinRMS[5] <- NA
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
# pre-pruning
# code from INFO304, which is teaching near identical content to STAT312
build.model <- function(train, formula, maxdepth = 4){
rpart(formula,
data = train,
model = TRUE,
control = rpart.control(maxdepth = maxdepth,
minsplit = 2,
minbucket = 2,
cp = 0.0))
}
predict.model <- function(m, test){
predict(m, newdata = test, type = "vector")
}
md <- build.model(train = train, formula = logSalary~Hits+Years)
fancyRpartPlot(md)
# predict.model(m = md, test = test)
baseRMS[5] <- mean((test$logSalary-predict.model(m = md, test = test))^2)
colinRMS[5] <- NA
scores <- t(cbind(baseRMS,colinRMS))
colnames(scores) <- c("Tree", "Bagged", "Random", "Boosted", "rpart")
barplot(t(cbind(baseRMS,colinRMS)), beside = TRUE, names.arg = colnames(scores), legend.text=c("base", "colinear removed")) +
title(main = "RMS score difference before and after co-linear variables are removed")
tree.bb = tree(logSalary~Hits+Years,data=train)
cat("unpruned tree RMS test error", mean((test$logSalary-predict(tree.bb))^2))
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19)
prune.bb = prune.tree(tree.bb,best=4)
baseRMS[1] <- mean((test$logSalary-predict(prune.bb))^2)
plot(prune.bb)
text(prune.bb)
prune.tree(tree.bb,best=10)
tree.bb = tree(logSalary~Hits+Years,data=trainMod)
pairs(hitters[,c("AtBat", "Hits", "Runs", "logSalary")])
pairs(hitters[,c("CAtBat", "CHits", "CRuns", "logSalary")])
hittersMod <- subset(hitters, select = -c(AtBat, Runs, CAtBat, CRuns))
trainMod <- hitters[train_ind, ]
testMod <- hitters[-train_ind, ]
tree.bb = tree(logSalary~Hits+Years,data=trainMod)
cv.bb = cv.tree(tree.bb,K=10)
plot(cv.bb$size, cv.bb$dev, type="b", pch=19, main = "optimal tree size")
prune.bb = prune.tree(tree.bb,best=4)
colinRMS[1] <- mean((testMod$logSalary-predict(prune.bb))^2)
a.s <- 3
printPrime <-  function(upperB){
i <- 2
while(i != upperB){
prime <- TRUE
j <- 2
while(j != i){
if(i % j == 0){prime <- FALSE}
}
j <- j + 1
} if (prime) {
print(i)
}
i <- i + 1
}
2%2
2%/%2
4%/%2
4%%2
printPrime <-  function(upperB){
i <- 2
while(i != upperB){
prime <- TRUE
j <- 2
while(j != i){
if(i %% j == 0){prime <- FALSE}
}
j <- j + 1
} if (prime) {
print(i)
}
i <- i + 1
}
printPrime <-  function(upperB){
i <- 2
while(i != upperB){
prime <- TRUE
j <- 2
while(j != i){
if(i %% j == 0){prime <- FALSE}
}
j <- j + 1
}
if (prime) {
print(i)
}
i <- i + 1
}
printPrime(2)
printPrime(3)
prime <- TRUE
j <- 2
j!=3
3 %% j == 0
j <- j + 1
prime
printPrime <-  function(upperB){
i <- 2
while(i != upperB){
prime <- TRUE
j <- 2
while(j != i){
if(i %% j == 0){prime <- FALSE}
}
j <- j + 1
}
if (prime) {
print(i)
print(prime)
}
i <- i + 1
}
printPrime(2)
printPrime(2)
printPrime(3)
printPrime(4)
R CMD check
setwd("~/GitHub/NZPoliceUtilities")
devtools::load_all()
build()
devtools::build()
devtools::install_local(path = "C:/Users/64223/Documents/GitHub/NZPoliceUtilities_0.0.0.9000.tar.gz")
NZPoliceUtilities::makeCodes()
devtools::build()
